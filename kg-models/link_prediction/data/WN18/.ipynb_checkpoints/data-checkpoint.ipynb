{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('03964744', '_hyponym', '04371774'),\n",
      " ('00260881', '_hypernym', '00260622'),\n",
      " ('02199712', '_member_holonym', '02188065'),\n",
      " ('01332730', '_derivationally_related_form', '03122748'),\n",
      " ('06066555', '_derivationally_related_form', '00645415')]\n"
     ]
    }
   ],
   "source": [
    "def read_triples(path):\n",
    "    triples = []\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f.readlines():\n",
    "            s, p, o = line.split()\n",
    "            triples += [(s.strip(), p.strip(), o.strip())]\n",
    "    return triples\n",
    "\n",
    "triples_tr = read_triples('./wn18/train.txt')\n",
    "pprint.pprint(triples_tr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40943 18 141442\n"
     ]
    }
   ],
   "source": [
    "entity_set = {s for (s, p, o) in triples_tr} | {o for (s, p, o) in triples_tr}\n",
    "predicate_set = {p for (s, p, o) in triples_tr}\n",
    "\n",
    "nb_entities, nb_predicates = len(entity_set), len(predicate_set)\n",
    "nb_examples = len(triples_tr)\n",
    "print(nb_entities, nb_predicates, nb_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_also_see': 0,\n",
      " '_derivationally_related_form': 1,\n",
      " '_has_part': 2,\n",
      " '_hypernym': 3,\n",
      " '_hyponym': 4,\n",
      " '_instance_hypernym': 5,\n",
      " '_instance_hyponym': 6,\n",
      " '_member_holonym': 7,\n",
      " '_member_meronym': 8,\n",
      " '_member_of_domain_region': 9,\n",
      " '_member_of_domain_topic': 10,\n",
      " '_member_of_domain_usage': 11,\n",
      " '_part_of': 12,\n",
      " '_similar_to': 13,\n",
      " '_synset_domain_region_of': 14,\n",
      " '_synset_domain_topic_of': 15,\n",
      " '_synset_domain_usage_of': 16,\n",
      " '_verb_group': 17}\n"
     ]
    }
   ],
   "source": [
    "entity_to_idx = {entity: idx for idx, entity in enumerate(sorted(entity_set))}\n",
    "predicate_to_idx = {predicate: idx for idx, predicate in enumerate(sorted(predicate_set))}\n",
    "pprint.pprint(predicate_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c8da79ba225f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpredicate_embedding_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mentity_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ent_embed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnb_entities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_embedding_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpredicate_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pre_embed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnb_predicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicate_embedding_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "entity_embedding_size = 150\n",
    "predicate_embedding_size = 150\n",
    "\n",
    "entity_embed = tf.get_variable('ent_embed', shape=[nb_entities, entity_embedding_size])\n",
    "\n",
    "predicate_embed = tf.get_variable('pre_embed', shape=[nb_predicates, predicate_embedding_size])\n",
    "\n",
    "print(entity_embed, predicate_embed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
